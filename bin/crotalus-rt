#!/usr/bin/env python

# Python Standard Library
import argparse
import json
import logging
from threading import Thread
import time

# Other dependencies
import numpy as np
from obspy import UTCDateTime, Stream
from obspy.clients.fdsn import Client
import psycopg2

# Local files
from crotalus.config.database import (
    continuous_extraction_channels, query_feature_conf, query_general_conf
)
from crotalus.dsp.features import (
    dsar, freq_central, freq_centroid, freq_domi, freq_ratio, kurtosis, rsem,
    tonality
)
from crotalus.dsp.pre_process import pre_process
from crotalus.dsp.spectrum import (
    downsample_spectrogram, get_8ve_bands, spectrum
)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('jsonfile', help='JSON file with database information')
    return parser.parse_args()


def connect_waveserver(ip, port):
    url = f'http://{ip}:{port}'
    try:
        logging.info(f'Connecting to {url}...')
        client = Client(url)
        logging.info('Succesfully connected to FDSN client.\n')
        return client
    except Exception as e:
        logging.info(e)


def get_waveforms(client, channels, starttime, endtime):
    def _get_waveform(network, station, channel, starttime, endtime):
        nonlocal st
        st += client.get_waveforms(
            network, station, '*', channel,
            starttime, endtime, attach_response=True
        )

    threads, st = [], Stream()
    for i, row in channels.iterrows():
        thread = Thread(
            target=_get_waveform,
            args=(
                row.network,
                row.station,
                row.channel,
                starttime,
                endtime
            )
        )
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()
    return st


def process(tr, conn, cg, cf, midtime, channel_id):
    # Time series features
    _rsem     = rsem(tr.data)
    _kurtosis = kurtosis(tr.data)
    _dsar     = dsar(tr, cf.dsar.freqmin, cf.dsar.freqmax, cf.dsar.order,
                     tr.stats.npts/tr.stats.sampling_rate, 0)[1][0]

    # Spectral features
    f, Sx = spectrum(tr, cg.pad)

    _freq_central  = freq_central(f, Sx)
    _freq_centroid = freq_centroid(f, Sx)
    _freq_domi     = freq_domi(f, Sx, 1)
    _freq_ratio    = freq_ratio(f, Sx, cf.freq_ratio.freqmin, cf.freq_ratio.freqmax)
    _freq_top_k    = freq_domi(f, Sx, cf.freq_top_k.k)

    fc, _ssam      = downsample_spectrogram(
        f, Sx, cf.ssam.f_lower, cf.ssam.f_upper, method=cf.ssam.method,
        fraction=cf.ssam.fraction, sampling_rate=tr.stats.sampling_rate
    )

    _tonality      = tonality(f, Sx, cf.tonality.k, cf.tonality.bin_width,
                              tr.stats.sampling_rate)

    with conn:
        c = conn.cursor()
        query = """
        INSERT INTO
            continuous (
                channel_id,
                time,
                rsem,
                ssam,
                dsar,
                freq_domi,
                freq_top_k,
                freq_central,
                freq_centroid,
                kurtosis,
                tonality,
                freq_ratio
            )
        VALUES
            (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);
        """
        values = (
            int(channel_id),
            midtime.datetime,
            float(_rsem),
            list(_ssam),
            float(_dsar),
            float(_freq_domi),
            float(_freq_top_k),
            float(_freq_central),
            float(_freq_centroid),
            float(_kurtosis),
            float(_tonality),
            float(_freq_ratio)
        )
        c.execute(query, values)


def main():
    args = parse_args()
    with open(args.jsonfile) as f:
        auth = json.load(f)

    conn = psycopg2.connect(**auth['database'])

    cg = query_general_conf(conn)
    cf = query_feature_conf(conn)

    channels = continuous_extraction_channels(conn)

    client = connect_waveserver(auth['fdsn']['ip'], auth['fdsn']['port'])

    e = UTCDateTime.now()
    endtime = UTCDateTime(
        year=e.year, month=e.month, day=e.day, hour=e.hour, minute=e.minute
    ) + cg.window_length / 2 # - 1000
    starttime = endtime - cg.window_length

    while True:
        _starttime = str(starttime).split('.')[0]
        _endtime   = str(endtime).split('.')[0]
        logging.info(f'Downloading waves for {_starttime}-{_endtime}...')

        st = get_waveforms(client, channels, starttime, endtime)
        midtime = starttime + (endtime - starttime) / 2

        logging.info('Pre-processing waves...')
        pre_process(
            st, int(cg.decimation_factor), cg.freqmin, cg.freqmax, cg.order,
            cg.multiple
        )

        logging.info('Processing waves...')
        for tr in st:
            channel_id = channels[
                (channels.station == tr.stats.station) &
                (channels.channel == tr.stats.channel)
            ].iloc[0].id

            process(tr, conn, cg, cf, midtime, channel_id)

        logging.info('Done.\n')

        starttime += cg.step
        endtime   += cg.step

        now = UTCDateTime.now()

        if now > endtime:
            continue
        else:
            waiting_time = endtime - now
            logging.info(
                f'Buffering next window, waiting time: {waiting_time:.2f} s...'
            )
            time.sleep(waiting_time)


if __name__ == '__main__' :
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    main()
